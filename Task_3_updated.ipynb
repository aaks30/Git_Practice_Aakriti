{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaks30/Git_Practice_Aakriti/blob/main/Task_3_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 3: Text Prediction**\n"
      ],
      "metadata": {
        "id": "acvrFntO7Qaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I built and trained a **Word-Level LSTM** model to generate text based on a dataset from Project Gutenberg. Below is a summary of the steps involved in processing the data, training the model, and generating text:"
      ],
      "metadata": {
        "id": "P5R7xqU07PDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Achievements:**\n",
        "- **Data Preprocessing**:\n",
        "  - Loaded the Project Gutenberg text data.\n",
        "  - Removed unnecessary metadata and irrelevant sections like the table of contents.\n",
        "  - Cleaned the text by removing special characters, numbers, and excessive spaces.\n",
        "- **Text Tokenization**:\n",
        "  - Tokenized the cleaned text into individual words for further processing.\n",
        "- **Vocabulary Building**:\n",
        "  - Built a vocabulary mapping (stoi: string-to-index and itos: index-to-string) based on word frequencies.\n",
        "- **Model Development**:\n",
        "  - Implemented a Word-Level LSTM (Long Short-Term Memory) model for text generation.\n",
        "  - Designed the model with embedding layers, LSTM layers, and fully connected layers for predicting the next word.\n",
        "- **Training**:\n",
        "  - Trained the model using the cleaned and tokenized dataset.\n",
        "  - Used Adam optimizer and CrossEntropyLoss for training.\n",
        "- **Model Saving**:\n",
        "  - Saved the trained model weights to ensure reproducibility and for future use.\n",
        "  - Saved the vocabulary mappings (stoi, itos) for the generation of text from new seed inputs.\n",
        "  \n",
        "### **Types of Approaches:**\n",
        "- **LSTM (Long Short-Term Memory)**:\n",
        "  - Effective for sequential data, ideal for text generation where the model learns dependencies between words over long sequences.\n",
        "- **GRU (Gated Recurrent Unit)**:\n",
        "  - A simpler alternative to LSTM, often faster with fewer parameters while still capturing temporal dependencies in sequences.\n",
        "- **Bidirectional LSTM**:\n",
        "  - Captures context from both past and future words in the text by processing the sequence in both directions.\n",
        "- **Transformer-based Models**:\n",
        "  - Modern architectures like GPT (Generative Pretrained Transformer) can capture longer-range dependencies and perform better in generating more coherent text.\n",
        "- **Character-Level Text Generation**:\n",
        "  - Instead of generating text word by word, this approach generates text one character at a time, capturing finer details of word formation.\n",
        "- **RNN (Recurrent Neural Network)**:\n",
        "  - An earlier approach for sequential data tasks. Though less effective than LSTMs, it can still be used for basic text generation tasks.\n"
      ],
      "metadata": {
        "id": "l8pdSlru46wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "4FotaBreeTpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load text file\n",
        "# Function to load text file contents into memory\n",
        "def load_text(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text = file.read()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "Dlmu5U-ieVMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove metadata from Project Gutenberg books\n",
        "# This function removes unnecessary metadata (headers/footers) from the text\n",
        "def remove_metadata(text):\n",
        "    start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
        "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "\n",
        "    start_idx = text.find(start_marker)\n",
        "    end_idx = text.find(end_marker)\n",
        "\n",
        "    if start_idx != -1 and end_idx != -1:\n",
        "        text = text[start_idx + len(start_marker): end_idx].strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "5DxYQ2cQeWXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean text by removing numbers and special characters\n",
        "# This function standardizes text by making it lowercase and removing unwanted characters\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z0-9,.'!?;:()\\-\\s]\", \"\", text)  # Keep important punctuation\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
        "    return text"
      ],
      "metadata": {
        "id": "ceVGofiTeXfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract only the main story portion from the text (if applicable)\n",
        "# This function ensures the text starts from the main story by searching for specific keywords\n",
        "def extract_main_story(text):\n",
        "    story_start_keywords = [\"the adventure of the western star\", \"i was standing at the window\"]\n",
        "\n",
        "    for keyword in story_start_keywords:\n",
        "        story_start_idx = text.find(keyword)\n",
        "        if story_start_idx != -1:\n",
        "            return text[story_start_idx:]\n",
        "    return text"
      ],
      "metadata": {
        "id": "lMygTV9-ekLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text into individual words\n",
        "# This function splits the text into words based on whitespace\n",
        "def tokenize_text(text):\n",
        "    return text.split()"
      ],
      "metadata": {
        "id": "cOjk85cUesmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary from the tokenized words\n",
        "# The function builds two mappings: one from word to index (stoi) and one from index to word (itos)\n",
        "def build_vocab(tokenized_words):\n",
        "    word_counts = Counter(tokenized_words)  # Count the frequency of each word\n",
        "    sorted_vocab = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)  # Sort by frequency\n",
        "    stoi = {word: i+1 for i, (word, _) in enumerate(sorted_vocab)}  # Index starts from 1 (not 0)\n",
        "    itos = {i: word for word, i in stoi.items()}\n",
        "    return stoi, itos"
      ],
      "metadata": {
        "id": "TbAFIjYfevSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert words into indices using the built vocabulary\n",
        "# This function takes a list of tokenized words and converts each word into its corresponding index\n",
        "def words_to_indices(tokenized_words, stoi):\n",
        "    return [stoi[word] for word in tokenized_words if word in stoi]  # Only include words in the vocabulary"
      ],
      "metadata": {
        "id": "H8brOLL5ezBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate sequences for training based on a fixed sequence length\n",
        "# The function creates input-target pairs for model training\n",
        "def generate_sequences(indexed_text, seq_length=30):\n",
        "    input_sequences = []\n",
        "    target_words = []\n",
        "\n",
        "    for i in range(len(indexed_text) - seq_length):\n",
        "        input_sequences.append(indexed_text[i:i + seq_length])\n",
        "        target_words.append(indexed_text[i + seq_length])\n",
        "\n",
        "    # Convert sequences to PyTorch tensors\n",
        "    X = torch.tensor(input_sequences, dtype=torch.long)\n",
        "    Y = torch.tensor(target_words, dtype=torch.long)\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "yQuRsWu2e2_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Word-Level LSTM model for text generation\n",
        "# The model consists of an embedding layer, an LSTM layer, and a fully connected output layer\n",
        "class WordLevelLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2):\n",
        "        super(WordLevelLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embed input words into a dense vector space\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)  # LSTM for sequence learning\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)  # Output layer to predict next word in sequence\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        out = self.fc(lstm_out[:, -1, :])  # Use only the last LSTM output for prediction\n",
        "        return out"
      ],
      "metadata": {
        "id": "yWIk08x4e6Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function to optimize model's weights\n",
        "# The function trains the model using cross-entropy loss and backpropagation\n",
        "def train_model(model, dataloader, optimizer, criterion, epochs=10, device='cpu'):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_X, batch_Y in dataloader:\n",
        "            batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_X)  # Forward pass\n",
        "            loss = criterion(output, batch_Y)  # Calculate loss\n",
        "            loss.backward()  # Backpropagate the gradients\n",
        "            optimizer.step()  # Update the model parameters\n",
        "            total_loss += loss.item()  # Track total loss for monitoring\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "1gvzDicNe_jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text and generate data loader for training\n",
        "# The function reads the text file, cleans and tokenizes the text, and generates training sequences\n",
        "def preprocess_text(file_path, seq_length=30, batch_size=64):\n",
        "    raw_text = load_text(file_path)\n",
        "    cleaned_text = remove_metadata(raw_text)\n",
        "    cleaned_text = clean_text(cleaned_text)\n",
        "    cleaned_text = extract_main_story(cleaned_text)\n",
        "    tokenized_words = tokenize_text(cleaned_text)\n",
        "    stoi, itos = build_vocab(tokenized_words)\n",
        "    indexed_text = words_to_indices(tokenized_words, stoi)\n",
        "    X, Y = generate_sequences(indexed_text, seq_length)\n",
        "\n",
        "    dataset = TensorDataset(X, Y)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return dataloader, stoi, itos"
      ],
      "metadata": {
        "id": "7euYOgbrfDW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File path for the text data\n",
        "file_path = \"/content/61262-0.txt\"\n",
        "dataloader, stoi, itos = preprocess_text(file_path, seq_length=30, batch_size=64)\n",
        "vocab_size = len(stoi) + 1  # Adding 1 for padding index"
      ],
      "metadata": {
        "id": "haqbwTpFfG03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = WordLevelLSTM(vocab_size)\n",
        "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer for training"
      ],
      "metadata": {
        "id": "N9s4fo_MfIXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "train_model(model, dataloader, optimizer, criterion, epochs=10)\n",
        "\n",
        "# Print the model architecture to inspect the layers\n",
        "print(model)"
      ],
      "metadata": {
        "id": "0GIj1STBfKZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model weights for future use\n",
        "torch.save(model.state_dict(), \"word_level_lstm.pth\")\n",
        "print(\"Model saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2I6FG7bUNNZq",
        "outputId": "5486a602-02e7-4f11-e7ef-787f79b89912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved!\n",
            "Model saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the vocabulary (stoi and itos) for text generation during inference\n",
        "import pickle\n",
        "with open(\"stoi.pkl\", \"wb\") as f:\n",
        "    pickle.dump(stoi, f)\n",
        "\n",
        "with open(\"itos.pkl\", \"wb\") as f:\n",
        "    pickle.dump(itos, f)"
      ],
      "metadata": {
        "id": "tW_cydfSP7A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "# Define the LSTM model class (same as during training)\n",
        "class WordLevelLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2):\n",
        "        super(WordLevelLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        out = self.fc(lstm_out[:, -1, :])  # Take the last output for prediction\n",
        "        return out\n",
        "\n",
        "# Load the vocabulary (stoi and itos)\n",
        "with open(\"stoi.pkl\", \"rb\") as f:\n",
        "    stoi = pickle.load(f)\n",
        "\n",
        "with open(\"itos.pkl\", \"rb\") as f:\n",
        "    itos = pickle.load(f)\n",
        "\n",
        "# Define vocab_size for the model\n",
        "vocab_size = len(stoi) + 1  # Add 1 for padding\n",
        "\n",
        "# Load the trained model\n",
        "model = WordLevelLSTM(vocab_size)\n",
        "model.load_state_dict(torch.load(\"word_level_lstm.pth\"))\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Generate new text based on a seed input\n",
        "def generate_text(model, stoi, itos, seed_text, max_words=50, temperature=1.0, device='cpu'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    words = seed_text.split()\n",
        "    for _ in range(max_words):\n",
        "        input_sequence = [stoi[word] for word in words[-30:] if word in stoi]  # Use the last 30 words as input\n",
        "        input_tensor = torch.tensor(input_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_tensor)  # Get predictions from the model\n",
        "            logits = logits / temperature  # Adjust temperature for randomness in generation\n",
        "            probabilities = torch.nn.functional.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
        "            predicted_index = torch.multinomial(probabilities, 1).item()  # Sample next word from the probabilities\n",
        "\n",
        "        next_word = itos.get(predicted_index, \"<UNK>\")  # Get the predicted word\n",
        "        words.append(next_word)\n",
        "\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "tYg4HsqhVq9G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "262af55e-fc1b-4ccf-e2bc-6bccdf1695af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-03fafa2cae30>:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"word_level_lstm.pth\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a sample text starting from a seed phrase\n",
        "generated_text = generate_text(model, stoi, itos, seed_text=\"wonderful person\", max_words=50)\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd2n-xP9QL7r",
        "outputId": "2e7c5539-8626-4d84-c902-66744c52805b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "wonderful person two pretty willard isnt us, in this pandemonium they reappear the door. mrs. opalsen died into the gun-room face, by the pace. it must have been no place, labelso then, red trying for the same old glory that poirot menacing. where she was stolen. it is being proved from these\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Performance Improvement Techniques:**\n",
        "1. **Using Pretrained Models (Transfer Learning)**:\n",
        "   - Leverage pretrained models like GPT or BERT for better context understanding and more accurate text generation.\n",
        "\n",
        "2. **Hyperparameter Tuning**:\n",
        "   - Experiment with hyperparameters like learning rate, batch size, and model dimensions to optimize performance and achieve the best results.\n",
        "\n",
        "3. **Bidirectional LSTM**:\n",
        "   - Implement a bidirectional LSTM to capture both past and future context, enhancing the model’s ability to generate more coherent and accurate predictions.\n"
      ],
      "metadata": {
        "id": "KbrRwNoR5_Uk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comparison with a Large Language Model (LLM):**\n",
        "- **Prompting LLMs for Human-Level Sentences**: Large Language Models like GPT-3 or GPT-4 generate human-level sentences by understanding vast amounts of context and patterns from a wide range of text data. They can generate highly coherent and semantically rich text based on the input prompt, without the need for explicit sequence-by-sequence training.\n",
        "\n",
        "- **Difference in Scale**: Unlike LSTMs, which are trained on specific datasets and may struggle with long-term dependencies, LLMs are pretrained on massive datasets with billions of parameters, enabling them to generate more diverse and contextually accurate text. My approach, based on an LSTM model, is limited by the size of the training data and model architecture.\n",
        "\n",
        "- **Improving My Model to Reach LLM-Level Performance**:\n",
        "   - **Pretraining on a Large Corpus**: Like LLMs, pretraining on a larger corpus of text data (such as from books, articles, etc.) and fine-tuning for specific tasks would allow my LSTM model to achieve more human-like text generation.\n",
        "   - **Larger Datasets**: To approach LLM performance, training on much larger, diverse datasets would improve the model’s ability to generalize and generate better text.\n",
        "   - **Advanced Architectures (Transformer)**: Switching from LSTM to Transformer-based architectures (like GPT or BERT) could also lead to much better results, as Transformers are known to handle long-range dependencies better than traditional RNNs or LSTMs.\n",
        "\n",
        "### **Comparision Table:**\n",
        "\n",
        "| **Aspect**                          | **LSTM Model (This Project)**                             | **ChatGPT**                                          |\n",
        "|-------------------------------------|----------------------------------------------------------|------------------------------------------------------|\n",
        "| **Text Coherence and Fluency**      | Generates simple, repetitive text.                       | Generates fluent, natural, and diverse text.         |\n",
        "| **Context Understanding**           | Struggles with long context and details.                  | Keeps track of long context and complex ideas.       |\n",
        "| **Creativity**                      | Limited creativity, repeats patterns.                     | Highly creative, generates varied responses.         |\n",
        "| **Grammar**                         | Mostly correct, some awkward phrasing.                    | Almost always perfect grammar and smooth structure.   |\n",
        "| **Text Diversity**                  | Limited, tends to repeat.                                 | Highly diverse, with different structures and words. |\n",
        "| **Training Data**                   | Trained on a small, specific dataset (Project Gutenberg). | Trained on a large, diverse dataset.                 |\n",
        "| **Model Complexity**                | Simple LSTM with two layers.                             | Complex model with billions of parameters.           |\n",
        "| **Real-Time Interaction**           | Generates text from a fixed input.                        | Interactive, adapts to ongoing conversation.         |\n",
        "| **Scope**                           | Best for specific tasks, like literature.                | General-purpose, handles many tasks (Q&A, writing).  |\n",
        "\n"
      ],
      "metadata": {
        "id": "EqjE7fAM6ZaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion:**\n",
        "- The LSTM-based model has successfully demonstrated text generation capabilities, providing a foundational approach for predicting the next word in a sequence. While this model performs reasonably well on smaller datasets, it faces limitations in generating highly coherent and contextually rich text compared to state-of-the-art Large Language Models like GPT-3 or GPT-4.\n",
        "- Key improvements, such as hyperparameter tuning, pretraining on larger datasets, and the potential shift to Transformer architectures, can significantly enhance the performance and bring the model closer to achieving human-level text generation.\n",
        "- Overall, this project highlights the importance of model design, data quality, and training strategies in developing effective text prediction systems. Further exploration into advanced architectures and pretraining techniques can elevate the text generation capabilities of future models.\n"
      ],
      "metadata": {
        "id": "3He-I6O46_GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iultnsf8QOjK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}